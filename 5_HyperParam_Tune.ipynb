{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TimeSeriesData' from 'src' (/Users/jacobtutt/Desktop/MPhil_DIS/M2/Coursework_M2/src/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesData, load_qwen_model, add_lora, preprocessor, evaluate, EarlyStopping\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model_training_flops\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TimeSeriesData' from 'src' (/Users/jacobtutt/Desktop/MPhil_DIS/M2/Coursework_M2/src/__init__.py)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from accelerate import Accelerator\n",
    "from src import TimeSeriesData, load_qwen_model, add_lora, preprocessor, evaluate, EarlyStopping\n",
    "from src import model_training_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returned the base Qwen model without modification (rank = 0).\n"
     ]
    }
   ],
   "source": [
    "full_model, tokenizer, device = full_model(lora_rank=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import preprocessor\n",
    "from src import full_model\n",
    "from src import train\n",
    "from src import TimeSeriesData\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "INFO - File loaded successfully. Trajectories shape: (1000, 100, 2), Time points shape: (100,)\n",
      "INFO - Scaling data by alpha=0.25283724069595337, ensuring 90% of values fit within the model's expected range.\n",
      "INFO - Data scaled to 3 decimal places\n",
      "INFO - Splitting the data into training, validation, and test sets with fractions: 0.7, 0.15, 0.15000000000000005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returning Qwen model injected with LoRA into Query and Value Projections with rank = 1\n",
      "Model loaded on mps\n"
     ]
    }
   ],
   "source": [
    "# Load model and tokeniser\n",
    "model, tokeniser, device = full_model(lora_rank=2)\n",
    "\n",
    "# Load and tokenise dataset\n",
    "train_set_total, val_set_total, test_set_total = preprocessor('lotka_volterra_data.h5', percentile=90, decimal_places=3, train_fraction=0.7, validation_fraction=0.15, shuffle=False, print_summary=False)\n",
    "\n",
    "\n",
    "train_dataset = TimeSeriesData(train_set_total, tokeniser, max_length=512, stride=500/2)\n",
    "val_dataset = TimeSeriesData(val_set_total, tokeniser, max_length=512, stride=512)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training:   2%|‚ñè         | 2/100 [02:01<1:44:38, 64.07s/step]"
     ]
    }
   ],
   "source": [
    "model, step_tracker, val_loss_tracker, total_flops = train(model, lora_rank=1, max_training_steps=100, batch_size=2, learning_rate=1e-4, train_loader = train_loader , val_loader = val_loader, early_stopping_patience=3, subset = 5, print_summary=True, wandb_run=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_once(config):\n",
    "    # Load model and tokenizer\n",
    "    model, tokeniser, device = load_qwen_model()\n",
    "    add_lora(model, config[\"lora_rank\"])\n",
    "\n",
    "    # Extract config\n",
    "    learning_rate = config[\"learn_rate\"]\n",
    "    lora_rank = config[\"lora_rank\"]\n",
    "    max_training_steps = config[\"max_steps\"]\n",
    "    no_train_sequences = config[\"train_sequences\"]\n",
    "    no_val_sequences = config[\"val_sequences\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    decimal_places = config[\"decimal_places\"]\n",
    "    token_length = config[\"token_length\"]\n",
    "\n",
    "    # Preprocess\n",
    "    train_set_total, val_set_total, _ = preprocessor(\n",
    "        'lotka_volterra_data.h5',\n",
    "        percentile=90,\n",
    "        decimal_places=decimal_places,\n",
    "        train_fraction=0.7,\n",
    "        validation_fraction=0.15,\n",
    "        shuffle=False,\n",
    "        print_summary=False\n",
    "    )\n",
    "\n",
    "    train_set = train_set_total[:no_train_sequences]\n",
    "    val_set = val_set_total[:no_val_sequences]\n",
    "\n",
    "    train_dataset = TimeSeriesData(train_set, tokeniser)\n",
    "    val_dataset = TimeSeriesData(val_set, tokeniser)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "\n",
    "    optimiser = torch.optim.Adam((p for p in model.parameters() if p.requires_grad), lr=learning_rate)\n",
    "    early_stopper = EarlyStopping(patience=5, mode='min')\n",
    "\n",
    "    accelerator = Accelerator()\n",
    "    model, optimiser, train_loader, val_loader = accelerator.prepare(model, optimiser, train_loader, val_loader)\n",
    "\n",
    "    model.train()\n",
    "    step = 0\n",
    "    total_eval_cost = 0\n",
    "\n",
    "    pbar = tqdm(total=max_training_steps, desc=\"Training\")\n",
    "\n",
    "    while step < max_training_steps:\n",
    "        for batch in train_loader:\n",
    "            optimiser.zero_grad()\n",
    "            outputs = model(batch, labels=batch)\n",
    "            loss = outputs.loss\n",
    "            accelerator.backward(loss)\n",
    "            optimiser.step()\n",
    "\n",
    "            step += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                val_loss, flops_cost = evaluate(model, val_loader, accelerator, lora_rank)\n",
    "                total_eval_cost += flops_cost\n",
    "                pbar.set_postfix({\"train_loss\": loss.item(), \"val_loss\": val_loss})\n",
    "\n",
    "                early_stopper(val_loss)\n",
    "                if early_stopper.early_stop:\n",
    "                    print(f\"\\nüõë Early stopping at step {step}\")\n",
    "                    pbar.close()\n",
    "                    return val_loss\n",
    "\n",
    "            if step >= max_training_steps:\n",
    "                break\n",
    "\n",
    "    pbar.close()\n",
    "    training_flops, _ = model_training_flops(no_tokens=token_length, lora_ranks=lora_rank, batch_size=batch_size, num_steps_training=step, print_summary=False)\n",
    "    print(f'\\nüìä Total training cost: {training_flops:.2e} FLOPs')\n",
    "    print(f'üìä Total evaluation cost: {total_eval_cost:.2e} FLOPs')\n",
    "    print(f\"\\n‚úÖ Training completed. Final validation loss: {val_loss:.4f}\")\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently loading: Qwen/Qwen2.5-0.5B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "INFO - File loaded successfully. Trajectories shape: (1000, 100, 2), Time points shape: (100,)\n",
      "INFO - Scaling data by alpha=0.25283724069595337, ensuring 90% of values fit within the model's expected range.\n",
      "INFO - Data scaled to 3 decimal places\n",
      "INFO - Splitting the data into training, validation, and test sets with fractions: 0.7, 0.15, 0.15000000000000005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Training:  10%|‚ñà         | 10/100 [00:29<04:01,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|‚ñà‚ñà        | 20/100 [01:23<04:01,  3.02s/it, train_loss=4.05, val_loss=3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|‚ñà‚ñà‚ñà       | 30/100 [02:16<03:37,  3.11s/it, train_loss=4.16, val_loss=2.31]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|‚ñà‚ñà‚ñà‚ñà      | 40/100 [03:10<02:52,  2.87s/it, train_loss=2.16, val_loss=1.75]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 50/100 [04:20<02:50,  3.41s/it, train_loss=1.24, val_loss=1.44]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [05:14<01:48,  2.72s/it, train_loss=1.14, val_loss=1.29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 70/100 [06:11<01:29,  2.98s/it, train_loss=1.25, val_loss=1.21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [07:09<01:04,  3.22s/it, train_loss=1.17, val_loss=1.17]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 90/100 [08:05<00:27,  2.78s/it, train_loss=1.12, val_loss=1.14]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [08:56<00:00,  2.79s/it, train_loss=0.563, val_loss=1.11]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [09:21<00:00,  5.61s/it, train_loss=1.17, val_loss=1.09] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Total training cost: 3.62e+14 FLOPs\n",
      "üìä Total evaluation cost: 3.08e+14 FLOPs\n",
      "\n",
      "‚úÖ Training completed. Final validation loss: 1.0922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_config = {\n",
    "    \"learn_rate\": 1e-4,\n",
    "    \"lora_rank\": 4,\n",
    "    \"max_steps\": 100,\n",
    "    \"train_sequences\": 200,\n",
    "    \"val_sequences\": 10,\n",
    "    \"batch_size\": 2,\n",
    "    \"decimal_places\": 3,\n",
    "    \"token_length\": 512\n",
    "}\n",
    "\n",
    "val_loss = train_once(test_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M2_CW_Venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
