{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import load_qwen_model\n",
    "from src import forwards_pass_flops\n",
    "from src import model_generation_flops, model_training_flops, model_evaluation_flops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "model, tokeniser, device = load_qwen_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Included in this Notebook\n",
    "- An outline of the Qwen + LoRA architecture\n",
    "- A summary of the mathematical approach taken\n",
    "- Examples from the defined function used to calculate flops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break Down of Qwen Achitecture and Flops Calculation\n",
    "\n",
    "## And flops Budget plan at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture:\n",
    "\n",
    "    - Token to Embedding Layer (no compute)\n",
    "    - 24 Transformer Layers:\n",
    "            - RMSNorm\n",
    "            - Rotary Positional Embedding (Query and Value Heads)\n",
    "            - Grouped Query Attention/ Multi Head Attention (14 Query Heads, 2 Key Heads, 2 Value Heads)\n",
    "                    - Query, Key, Value Heads + LoRA Rank Adaptation if included\n",
    "                    - Attention Mechanism\n",
    "                    - Softmax\n",
    "                    - Self Attention Mechanism (Values * Softmax)\n",
    "                    - Concatination\n",
    "                    - Linear Transformation (Mixing)\n",
    "            - Residual Connection\n",
    "            - RSMNorm\n",
    "            - MLP (Projection Up, SwiGLU Activation Function, Projection Down)\n",
    "            - Residual Connection\n",
    "    - RMSNorm\n",
    "    - Embedding to Vocabulary Linear Layer\n",
    "----\n",
    "For probabilitic generation:\n",
    "\n",
    "    - Softmax over Vocabuluary Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture of Qwen](diagrams/QwenModel.jpg)\n",
    "\n",
    "- note the rotary positional embeddings are applied to the query and key values internally not beforehand but are shown here as markers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Calculations\n",
    "\n",
    "The majority of the computations in the following analysis can be grouped as **matrix multiplications**. \n",
    "\n",
    "For a matrix multiplication of dimensions:  \n",
    "**(D, S) × (S, R) → (D, R)**\n",
    "\n",
    "The number of floating-point operations required are:\n",
    "\n",
    "- **Multiplications**:  \n",
    "  $ D \\times S \\times R $\n",
    "\n",
    "- **Additions**:  \n",
    "  $ D \\times (S - 1) \\times R $\n",
    "\n",
    "> Each output element in the resulting matrix requires `S` multiplications and `S - 1` additions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen (Without LoRA)\n",
    "- The Lora additional structure is introduced later.\n",
    "\n",
    "---\n",
    "\n",
    "## Before the Transformer Layers\n",
    "### Convert Tokens to Embedding:\n",
    "- This is treated as a memory operation in the forwards pass (although this mapping is adjusted during the backward pass) it is simply a mapping of key to embedding vector\n",
    "- It does not add to the flops budget given the assumtion (backlwards pass = 2x Forwards Pass)\n",
    "\n",
    "---\n",
    "\n",
    "## Tranformer Layer: 24 of them\n",
    "### RMS Norm\n",
    "- RMS works by calculating the squared mean of each input element and calculating its squared mean along the embedding space.\n",
    "- Each input element is then divided by its repective embedding RMS and a constant is then applied.\n",
    "- This is applied before and after self attention in each layer, as well as once again after all 24 transformer layers.\n",
    "The **Root Mean Square (RMS)** is defined as:\n",
    "\n",
    "\n",
    "$$ \\text{RMS}(\\mathbf{a}) = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} a_i^2 } $$\n",
    "\n",
    "Each normalised element is:\n",
    "\n",
    "$$ \\bar{a}_i = \\frac{a_i}{\\text{RMS}(\\mathbf{a})} $$\n",
    "\n",
    "---\n",
    "\n",
    "## Multi Head (Grouped Query) Self Attention \n",
    "\n",
    "| Section   |   No Heads |\n",
    "|:----------|-----------:|\n",
    "| Query     |         14 |\n",
    "| Key       |          2 |\n",
    "| Value     |          2 |\n",
    "\n",
    "- The qwen model used grouped query attnention and this has a different number of query heads to key and value heads, however for this calculation it is assumed to be the same mechanism as Multi Head Attention.\n",
    "- We use 14 query heads each of dimension **No Tokens X Dimension Embedding Space/ No. Heads**\n",
    "\n",
    "### Rotary Positional Embeddings\n",
    "- The Qwen model does not use standard positional embeddings added onto the embedding space however applies rotary positional embedding to every query-key matrices in the self attention.\n",
    "- This acts as a rotation matrix multiplication in which it encodes relative position information while **preserving distance metrics**.\n",
    "- This does not include the FLOPS of calculating the rotation matrix (as intrusted) but does include its application to the original space.\n",
    "- This is treated as a simple matrix multiplication to the overall **global** query and key heads however it can also be thought of as applied internally within each head, which would acheive the same flops.\n",
    "- A function is provided for applying it to once matrix and it is reused twice per self attention block (query and value)\n",
    "\n",
    "We define the rotary embedding transformation as:\n",
    "\n",
    "$$\n",
    "f_{\\{q,k\\}}(x_m, m) = R_{\\Theta, m}^d \\, W_{\\{q,k\\}} x_m\n",
    "$$\n",
    "where the rotation matrix $ R_{\\Theta, m}^d $ is:\n",
    "\n",
    "$$\n",
    "R_{\\Theta, m}^d =\n",
    "\\begin{pmatrix}\n",
    "\\cos(m\\theta_1) & -\\sin(m\\theta_1) & 0 & 0 & \\cdots & 0 \\\\\n",
    "\\sin(m\\theta_1) & \\cos(m\\theta_1)  & 0 & 0 & \\cdots & 0 \\\\\n",
    "0 & 0 & \\cos(m\\theta_2) & -\\sin(m\\theta_2) & \\cdots & 0 \\\\\n",
    "0 & 0 & \\sin(m\\theta_2) & \\cos(m\\theta_2)  & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & 0 & \\cdots & \\cos(m\\theta_{d/2}) & -\\sin(m\\theta_{d/2}) \\\\\n",
    "0 & 0 & 0 & 0 & \\cdots & \\sin(m\\theta_{d/2}) & \\cos(m\\theta_{d/2}) \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- In this calculation for robustness i have included the computational flops by times by 0 in the matrix multiplication however in qwen architecture this may be simplified.\n",
    "\n",
    "### Query, Key, Value Matrices\n",
    "- The qwen model use of grouped query attention may mean that the flops is slightly different to what is applied, as with only 2 Key and Value heads, their dimensionality differs and are effectively reused in individual heads\n",
    "- Here we assume standard multi head attention, which makes calclulting the query, key value heads trivial with matrix multiplication\n",
    "- It is important to note that qwen models use Bias terms within these and this has been accounted for\n",
    "\n",
    "### Attention (inc softmax)\n",
    "- Tne attention is simply broken down across the 14 heads as a matrix multiplication\n",
    "- Additionally we accound for scaling by the dimensionality of the head. This is treated as a single square root as it can be stored in memory. But it is a division over the entire Attention results. \n",
    "- The softmax is applied to the attention matrix overall which is simply an exponential on every term in the input matrix (no_tokens * no_tokens), followed by the sum across the 1 dimension and finally a division. This is done across all 14 heads\n",
    "\n",
    "### Self Attention (Attention * Values)\n",
    "- This is a simply matrix multiplication (No Tokens, No Tokens) X (No, Tokens, Dension of Value Head)\n",
    "- Within the Masked Self-Attention operation, the upper triangular portion of the attention score matrix is masked out, as tokens are only allowed to attend to previous positions in the sequence. \n",
    "- The inner working of the multiplication whether the associated multiplication occour or are treated as a memory operation, to provide a conservative estimate the full cost is carried foward. \n",
    "\n",
    "### Concatination\n",
    "- Concatinating the results of each head is treated as a memory operation and thus does not require flops calculations\n",
    "\n",
    "### Linear Transformation (Mixing)\n",
    "- Denoted at the `o_proj): Linear(in_features=896, out_features=896, bias=False)` this is simple a linear tranformation at the end of the head concatination inorder to allow mixing of the information from each head\n",
    "- A simple matrix multiplication by (embbed dim, embedding dim)\n",
    "\n",
    "---\n",
    "\n",
    "## Post Self Attention\n",
    "\n",
    "### Residual Connectiom\n",
    "- Residual connections in Qwen allow the embedding to carry forward information from earlier layers, preserving gradient flow and enabling deeper architectures.\n",
    "- This is a simple element-wise addition over a matrix of shape (no tokens, embedding dim), combining the input and output of a sub-layer\n",
    "\n",
    "### RMS Norm\n",
    "- Another RMS norm is applied here, idenitcal to the last\n",
    "\n",
    "---\n",
    "\n",
    "## MLP (Feed Forward with swiGLU activation)\n",
    "- This calculation if preformed over two functions `mlp_flops`, `swiglu_flops`. \n",
    "- This combination of MLP and MLP can be thought of in two ways. \n",
    "- The simple base part of the MLP can be thought of as a projection up from embedding dimension to the dimension of the mlp hidden layer (4864), and then a projection back down. Note that none of these projection have biases in the qwen model. \n",
    "\n",
    "$$\n",
    "\\text{SwiGLU}(x, W, V, b, c, \\beta) = \\text{Swish}_\\beta(xW + b) \\odot (xV + c)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ x $ is the input,\n",
    "- $ W, V $ are projection weight matrices,\n",
    "- $ b, c $ are biases (0 in the qwen model)\n",
    "- $ \\odot $ denotes element-wise multiplication,\n",
    "- $ \\text{Swish}_\\beta(z) = \\frac{z}{1 + e^{-\\beta z}} $ is the generalized Swish activation.\n",
    "\n",
    "- I like to think of this as two up projection matrices (biases zero) giving two hidden layers, in which swish is applied to one of them and then they are element wise multiplied giving a single hidden layer configuration. This is then passed into the down projection\n",
    "- The functions and calculations are split accordingly:\n",
    "- `mlp_flops`:\n",
    "    - Calculates the flops of two projections up into this space which are then turned into one and one projection down\n",
    "- `swiglu_flops`:\n",
    "    - Calculates the flops of applying the swish to one of the higher dimensional layers and then the element wise multiplication\n",
    "    - Assume negation in exponetial factor has no cost\n",
    "\n",
    "### Residual Connection\n",
    "- Another Residual Connection is applied here, idenitcal to the last\n",
    "\n",
    "---\n",
    "\n",
    "## Outside of transformer\n",
    "\n",
    "### RMS Norm\n",
    "- A final RMS norm is applied here, idenitcal to the last\n",
    "\n",
    "### Vocabuluary Projection\n",
    "- Finally, there is a **linear projection (including bias)** that maps the model's output from the embedding space to the **vocabulary space**.\n",
    "- This step is used to produce **logits** for each token in the vocabulary, which represent unnormalized probabilities.\n",
    "- Specifically, we project from the **embedding dimension** (e.g., 896 in Qwen) to the **vocabulary size** (e.g., 151,936 tokens).\n",
    "\n",
    "---\n",
    "\n",
    "### **During training, evaluation and greedy (determininstic) sampling this is where the model stops**\n",
    "- The cross entropy loss is calculated outside of the model and uses the logits and we do not account for these flops\n",
    "- In greedy sampling it simply takes the vocab token with the highest value \n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "## For Probabilistic (Stochastic) Generation\n",
    "- Inorder for the model to generate based of a probabilistic distribution the logits must be normalised, this involves applying a softmax to the final steps vocabuluary dimension and sampling based of this. This is only applied in the `model_generation_flops` function later if randomness = true\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture of Qwen](diagrams/Lora_view.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addition of Lora Ranks\n",
    "- In this project with apply LoRA to the query projection and value projection layers of the attention mechanism \n",
    "- It can be thought of as a seperate channel of matrix multiplication (2 steps) and then added to the original proection matrices. \n",
    "- In this implementation, we the query and value heads as a global matricies across the mutliple self attention heads and thus is applied to the full matrix at once however this is the same computational cost as applying to to the individual head individually and summing flops across heads\n",
    "\n",
    "### Broken Down:\n",
    "- Matrix Multiplication 1: Input X Lora 1 - (N_tokens,Embed dim) x (Embed, LoRA_Ranks)\n",
    "- Matrix Multiplication 2: Result x Lora 2 -(N_tokens, LoRA_Ranks) * (LoRA_Ranks, Embed dim)\n",
    "- Addition: Sum on top of Frozen Weight Matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Aggregation\n",
    "- I will now explain how these individual functions are broken down into"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Forwards Pass\n",
    "\n",
    "- Aggregates all of the function above into a single forward pass of the full architecture, accepting lora ranks (if lora rank = 0 does not add any computational cost)\n",
    "- It does not include the cost of generation (ie softmax) as this is implemented in a layer function, a few examples are given below to show that LoRA's additional computational cost is minimial\n",
    "\n",
    "### Change with lora: lora effects it at 4 orders of magnitude lower than total\n",
    "\n",
    "### No LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation Cost                     Additions          Multiplications    Divisions          Exponentiations    Square Roots       Total             \n",
      "Single Attention Block             2.93e+09           2.94e+09           7.34e+06           3.67e+07           10                5.91e+09          \n",
      "Single MLP Block                   6.69e+09           6.7e+09            2.49e+06           2.49e+07           0                 1.34e+10          \n",
      "RMS, Residual etc                  1.83e+06           1.84e+06           9.19e+05           0                  1.02e+04          4.6e+06           \n",
      "Single Transformer Layer           9.63e+09           9.63e+09           1.07e+07           6.16e+07           1.02e+04          1.93e+10          \n",
      "LoRA cost in this Layer            0                  0                  0                  0                  0                 0                 \n",
      "Full Forward Pass                  3.01e+11           3.01e+11           2.58e+08           1.48e+09           2.51e+05          6.03e+11          \n",
      "\n",
      "Overall Total FLOPs:                6.0342e+11\n",
      "Percentage of Total FLOPs Budget:   0.00060342 %\n"
     ]
    }
   ],
   "source": [
    "total_forward_pass, total_lora = forwards_pass_flops(no_tokens = 512, lora_ranks = 0, print_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA: Rank 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation Cost                     Additions          Multiplications    Divisions          Exponentiations    Square Roots       Total             \n",
      "Single Attention Block             2.93e+09           2.94e+09           7.34e+06           3.67e+07           10                5.91e+09          \n",
      "Single MLP Block                   6.69e+09           6.7e+09            2.49e+06           2.49e+07           0                 1.34e+10          \n",
      "RMS, Residual etc                  1.83e+06           1.84e+06           9.19e+05           0                  1.02e+04          4.6e+06           \n",
      "Single Transformer Layer           9.63e+09           9.64e+09           1.07e+07           6.16e+07           1.02e+04          1.93e+10          \n",
      "LoRA cost in this Layer            1.83e+06           2.75e+06           0                  0                  0                 4.59e+06          \n",
      "Full Forward Pass                  3.01e+11           3.01e+11           2.58e+08           1.48e+09           2.51e+05          6.04e+11          \n",
      "\n",
      "Overall Total FLOPs:                6.0353e+11\n",
      "Percentage of Total FLOPs Budget:   0.00060353 %\n"
     ]
    }
   ],
   "source": [
    "total_forward_pass, total_lora = forwards_pass_flops(no_tokens = 512, lora_ranks = 1, print_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA: Rank 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation Cost                     Additions          Multiplications    Divisions          Exponentiations    Square Roots       Total             \n",
      "Single Attention Block             2.93e+09           2.94e+09           7.34e+06           3.67e+07           10                5.91e+09          \n",
      "Single MLP Block                   6.69e+09           6.7e+09            2.49e+06           2.49e+07           0                 1.34e+10          \n",
      "RMS, Residual etc                  1.83e+06           1.84e+06           9.19e+05           0                  1.02e+04          4.6e+06           \n",
      "Single Transformer Layer           9.63e+09           9.64e+09           1.07e+07           6.16e+07           1.02e+04          1.93e+10          \n",
      "LoRA cost in this Layer            7.34e+06           8.26e+06           0                  0                  0                 1.56e+07          \n",
      "Full Forward Pass                  3.01e+11           3.01e+11           2.58e+08           1.48e+09           2.51e+05          6.04e+11          \n",
      "\n",
      "Overall Total FLOPs:                6.0379e+11\n",
      "Percentage of Total FLOPs Budget:   0.00060379 %\n"
     ]
    }
   ],
   "source": [
    "total_forward_pass, total_lora = forwards_pass_flops(no_tokens = 512, lora_ranks = 4, print_summary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating from Model:\n",
    "- This is very computationally expensive\n",
    "- If we request multiple tokens as output, the model must **generate tokens autoregressively** — meaning it generates one token at a time.\n",
    "- At each step, the model:\n",
    "  1. Takes in the current input sequence (starting with the original prompt),\n",
    "  2. Produces logits over the vocabulary,\n",
    "  3. Selects or samples the next token (based on decoding strategy),\n",
    "  4. Appends the new token to the input and **feeds it back into the model** to generate the next.\n",
    "\n",
    "- So if I give it a context of 512 and also be 512 more, it has to:\n",
    "    1. Run model with context 512\n",
    "    2. Run model with context 513\n",
    "    3. Run model with context 514 \n",
    "    4. ...\n",
    "    5. Run model with Context 1023\n",
    "\n",
    "\n",
    "Additionally if we are using stohastic sampling it must add a final softmax to the output at each interation. \n",
    "\n",
    "These cacluations are included in: `model_generation_flops`\n",
    "\n",
    "## NOTE:\n",
    "- In this example running a context of 512 to generate 512 costs 4.7317e+14 flops, which is 0.5% of our computational budget and not feasible to do often\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy sampling, No Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs for generating 294 tokens: 3.7761e+14\n",
      "Total FLOPs from LoRA adaptation: 0\n",
      "Percentage of Total FLOPs Budget:   0.37761 %\n"
     ]
    }
   ],
   "source": [
    "total_flops, total_lora_flops = model_generation_flops(tokens_given = 900, tokens_generated = 294, lora_ranks = 0, randomness = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs for generating 512 tokens: 4.7317e+14\n",
      "Total FLOPs from LoRA adaptation: 0\n",
      "Percentage of Total FLOPs Budget:   0.47317 %\n"
     ]
    }
   ],
   "source": [
    "total_flops, total_lora_flops = model_generation_flops(tokens_given = 512, tokens_generated = 512, lora_ranks = 0, randomness = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy sampling, Lora = 5 (Small addition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs for generating 512 tokens: 4.7353e+14\n",
      "Total FLOPs from LoRA adaptation: 3.5481e+11\n",
      "Percentage of Total FLOPs Budget:   0.47353 %\n"
     ]
    }
   ],
   "source": [
    "total_flops, total_lora_flops = model_generation_flops(tokens_given = 512, tokens_generated = 512, lora_ranks = 5, randomness = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic sampling, No Lora (Negligible difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs for generating 512 tokens: 4.7317e+14\n",
      "Total FLOPs from LoRA adaptation: 0\n",
      "Percentage of Total FLOPs Budget:   0.47317 %\n"
     ]
    }
   ],
   "source": [
    "total_flops, total_lora_flops = model_generation_flops(tokens_given = 512, tokens_generated = 512, lora_ranks = 0, randomness = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model:\n",
    "- In training we simply use the assumption that the backwards pass is 2 * the forwards pass\n",
    "- We also do not account for the cross-entropy loss calculation which is assumed to be outside the model as per intstructures.\n",
    "- Therefore the model trains of logit/ vocabuluary space\n",
    "- It can account for a given batch and no of training steps through simple multiplication\n",
    "\n",
    "## Note:\n",
    "\n",
    "This implies we have 13,800 training periods (context 512 and batch_size, 4) in our model calculations - far far smaller than the suggested run times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs for training: 7.2455e+12\n",
      "Total FLOPs from LoRA adaptation: 4.4909e+09\n",
      "Percentage of Total FLOPs Budget:   0.0072455 %\n"
     ]
    }
   ],
   "source": [
    "total_flops, total_lora_flops = model_training_flops(no_tokens = 512, lora_ranks = 4, batch_size = 4, num_steps_training = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing for the training limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs for training: 9.9988e+16\n",
      "Total FLOPs from LoRA adaptation: 6.1975e+13\n",
      "Percentage of Total FLOPs Budget:   99.988 %\n"
     ]
    }
   ],
   "source": [
    "total_flops, total_lora_flops = model_training_flops(no_tokens = 512, lora_ranks = 4, batch_size = 4, num_steps_training = 13800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model:\n",
    "\n",
    "- When evaluating the model internally, we rely on the cross entropy loss and thus we simply are running the forward pass for a given number of batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs for evaluation: 6.0342e+13\n",
      "Total FLOPs from LoRA adaptation: 0\n",
      "Percentage of Total FLOPs Budget:   0.060342 %\n"
     ]
    }
   ],
   "source": [
    "valuation_flops, evaluation_lora_flops = model_evaluation_flops(no_tokens = 512, lora_ranks = 0 , batch_size = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (M2.5 Coursework)",
   "language": "python",
   "name": "m3cw5_py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
