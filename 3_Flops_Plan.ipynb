{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import model_generation_flops, model_training_flops, model_evaluation_flops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Planning the flops budget**\n",
    "## Pre Experimentation plan\n",
    "\n",
    "- The previous notebooks calculations have shown that we have a very limited flops budget (approximately 15,000 training steps (512 tokens and a batch size of 4))\n",
    "\n",
    "- The two major sources of flops are:\n",
    "    - Training the model\n",
    "    - Autoregressive Generation (this is no longer included)\n",
    "\n",
    "- These calculation all assume a lora rank of 4  (Lora makes very little difference on overall scale)\n",
    "\n",
    "## Post Experimentation\n",
    "\n",
    "- When running future evaluations, training and tests the functions have been built to save all metrics to .json files including a breakdown of the flops used\n",
    "- The functions calculate these dynamically ie account for the exact number of batches used to evaluate on, the exact number of steps taken considering early stopping etc\n",
    "- These can then be read automatcially by the `flops_in_folder` \n",
    "- A summary is provided at the top of each notebook and a final value at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLAN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model \n",
    "- Using the Test Set Cross Entropy Loss\n",
    "\n",
    "### Approx 0.2414% \n",
    "- I still assume evaluation loss is in the buget - this is preformed on 150 trajectories\n",
    "- Approx 450 batches of 512 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs for evaluation: 2.4137e+14\n",
      "Total FLOPs from LoRA adaptation: 0\n",
      "Percentage of Total FLOPs Budget:   0.24137 %\n"
     ]
    }
   ],
   "source": [
    "valuation_flops, evaluation_lora_flops = model_evaluation_flops(no_tokens = 512, lora_ranks = 0 , batch_size = 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model with MSE\n",
    "\n",
    "### Not accounted for (if it was - would be very large(180%))\n",
    "\n",
    "- To be able to show how errors accumulate over time,and give a direct and intuitive measure of the modelâ€™s performance on predicting actual values rather than token probabilities.\n",
    "- We will do this twice:\n",
    "    - The baseline untrained Qwen model \n",
    "    - The Fully Trained Optimised model\n",
    "\n",
    "- We do this with the full set of examples from the full training set (150 time series trajectories)\n",
    "\n",
    "### Change of flops limit - No longer account for autoregressive generation on FLOPS limit\n",
    "- I orginally had only planned to use this on 25 examples and for before and after as this would have been 23% of my flops budget \n",
    "- However it was later changed so that this was not included in the flops budget so it was ramped up to the full 150 on the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total FLOPs for generating 290 tokens: 3.9863e+14\n",
      "Total FLOPs from LoRA adaptation: 0\n",
      "Percentage of Total FLOPs Budget:   0.39863 %\n",
      "Total FLOPS for 25 examples before and after: 1.196e+17\n",
      "Percentage of Budget used: 179.38456605142653%\n"
     ]
    }
   ],
   "source": [
    "total_analysis_flop, _ = model_generation_flops(tokens_given = 970, tokens_generated = 290, lora_ranks = 0, randomness = False)\n",
    "\n",
    "# Using twice before and after the model is trained - 25 exmaples\n",
    "print(f\"Total FLOPS for 25 examples before and after: {(float(total_analysis_flop) * 2 * 150):.3e}\")\n",
    "print(f\"Percentage of Budget used: {(float(total_analysis_flop) * 2 * 150 / 1e17 * 150)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/ Tuning Plan\n",
    "### (Approx 6.4 %)\n",
    "\n",
    "## Training with the default hyperparameters\n",
    "- It was first considered building this cost within the hyperparameter tuning (ie using one of the runs)\n",
    "- However it was decided that hyperparameter tuning should be done with a token length of 256 (thus further reduces the overall cost)\n",
    "- Thus this excluded the default values (token length of 512) and must be run seperatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a single test the potenital flops of training steps\n",
      "Total FLOPs for training: 5.7964e+15\n",
      "Total FLOPs from LoRA adaptation: 3.5927e+12\n",
      "Percentage of Total FLOPs Budget:   5.7964 %\n",
      "\n",
      "For a single mid training evaluation (repeated up to 20 times)\n",
      "Total FLOPs for evaluation: 1.5095e+13\n",
      "Total FLOPs from LoRA adaptation: 9.3561e+09\n",
      "Percentage of Total FLOPs Budget:   0.015095 %\n",
      "\n",
      "For a final evaluation\n",
      "Total FLOPs for evaluation: 2.7171e+14\n",
      "Total FLOPs from LoRA adaptation: 1.6841e+11\n",
      "Percentage of Total FLOPs Budget:   0.27171 %\n",
      "\n",
      "For a Total evaluation:\n",
      "5.736e+14\n",
      "\n",
      "Total FLOPs of single test:\n",
      "6.370e+15\n",
      "\n",
      "Across all potenital tests:\n",
      "5.736e+14\n",
      "Percentage of Budget used: 6.370%\n"
     ]
    }
   ],
   "source": [
    "# Training if no early stopping\n",
    "print('For a single test the potenital flops of training steps')\n",
    "total_flops, _ = model_training_flops(no_tokens = 512, lora_ranks = 4, batch_size = 4, num_steps_training = 800)\n",
    "# Evaluation\n",
    "print('')\n",
    "print('For a single mid training evaluation (repeated up to 20 times)')\n",
    "valuation_flops_inter, _ = model_evaluation_flops(no_tokens = 512, lora_ranks = 4 , batch_size = 25)\n",
    "print('')\n",
    "print('For a final evaluation')\n",
    "valuation_flops_final, _ = model_evaluation_flops(no_tokens = 512, lora_ranks = 4 , batch_size = 450)\n",
    "print('')\n",
    "print('For a Total evaluation:')\n",
    "print(f\"{(20 * valuation_flops_inter + valuation_flops_final):.3e}\")\n",
    "print('')\n",
    "print(f\"Total FLOPs of single test:\")\n",
    "print(f\"{(20 * valuation_flops_inter + valuation_flops_final + total_flops):.3e}\")\n",
    "print('')\n",
    "print('Across all potenital tests:')\n",
    "# Times be 14 to account for the runs with 512 and 768 tokens\n",
    "print(f\"{((20 * valuation_flops_inter + valuation_flops_final)):.3e}\")\n",
    "print(f\"Percentage of Budget used: {((20 * valuation_flops_inter + valuation_flops_final + total_flops) / 1e17 * 100):.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "### (Approx 44% -  Hopefully Less)\n",
    "- For baseline testing I will use a token length of 256 as the self attention increases the compute with token length significantly and this can be final tuned at the end\n",
    "- I will attempt to reduce the hyperparameter search area in as efficient way as possible using sub experiments\n",
    "- However this budget will account for the worst case senario in which all hyperparameters combinations will have to be run, ie a grid search of **11 configurations**\n",
    "- If parameters can be rulled out it will allow for further tuning in different ways\n",
    "\n",
    "### **Up to 11**(Hopefully Less) configurations\n",
    "- Testing for a training period of up to 800 training periods\n",
    "- Using a batch size of 4 and 256 tokens\n",
    "- Evaluate every 25 steps on a subbatch of 25\n",
    "- Evaluate on the end with the full validation set (150 trajectories)\n",
    "- (Note in the code I multiple by 14 this is because 2 tests use token indexes of 512 and 768 leading to (equivelent of 3 runs of addition compute))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a single test the potenital flops of training steps\n",
      "Total FLOPs for training: 2.8416e+15\n",
      "Total FLOPs from LoRA adaptation: 1.7964e+12\n",
      "Percentage of Total FLOPs Budget:   2.8416 %\n",
      "\n",
      "For a single mid training evaluation (repeated up to 20 times)\n",
      "Total FLOPs for evaluation: 7.3999e+12\n",
      "Total FLOPs from LoRA adaptation: 4.678e+09\n",
      "Percentage of Total FLOPs Budget:   0.0073999 %\n",
      "\n",
      "For a final evaluation\n",
      "Total FLOPs for evaluation: 1.332e+14\n",
      "Total FLOPs from LoRA adaptation: 8.4205e+10\n",
      "Percentage of Total FLOPs Budget:   0.1332 %\n",
      "\n",
      "For a Total evaluation:\n",
      "2.812e+14\n",
      "\n",
      "Total FLOPs of single test:\n",
      "3.123e+15\n",
      "\n",
      "Across all potenital tests:\n",
      "3.937e+15\n",
      "Percentage of Budget used: 43.719%\n"
     ]
    }
   ],
   "source": [
    "# Training if no early stopping\n",
    "print('For a single test the potenital flops of training steps')\n",
    "total_flops, _ = model_training_flops(no_tokens = 256, lora_ranks = 4, batch_size = 4, num_steps_training = 800)\n",
    "# Evaluation\n",
    "print('')\n",
    "print('For a single mid training evaluation (repeated up to 20 times)')\n",
    "valuation_flops_inter, _ = model_evaluation_flops(no_tokens = 256, lora_ranks = 4 , batch_size = 25)\n",
    "print('')\n",
    "print('For a final evaluation')\n",
    "valuation_flops_final, _ = model_evaluation_flops(no_tokens = 256, lora_ranks = 4 , batch_size = 450)\n",
    "print('')\n",
    "print('For a Total evaluation:')\n",
    "print(f\"{(20 * valuation_flops_inter + valuation_flops_final):.3e}\")\n",
    "print('')\n",
    "print(f\"Total FLOPs of single test:\")\n",
    "print(f\"{(20 * valuation_flops_inter + valuation_flops_final + total_flops):.3e}\")\n",
    "print('')\n",
    "print('Across all potenital tests:')\n",
    "# Times be 14 to account for the runs with 512 and 768 tokens\n",
    "print(f\"{(14*(20 * valuation_flops_inter + valuation_flops_final)):.3e}\")\n",
    "print(f\"Percentage of Budget used: {(14* (20 * valuation_flops_inter + valuation_flops_final + total_flops) / 1e17 * 100):.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Training\n",
    "### Approx 48% (with early stopping hopefully less)\n",
    "- Training period of up to 4000 training periods (early stopping will likely get it before this)\n",
    "- Using a batch size of 4 and 512 tokens\n",
    "- Evaluate every 50 steps on a subbatch of 25\n",
    "- Evaluate on the end with full validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For all potential training steps (4000)\n",
      "Total FLOPs for training: 4.4323e+16\n",
      "Total FLOPs from LoRA adaptation: 2.6946e+13\n",
      "Percentage of Total FLOPs Budget:   44.323 %\n",
      "\n",
      "For a single mid training evaluation (repeated up to 160 times)\n",
      "Total FLOPs for evaluation: 2.3085e+13\n",
      "Total FLOPs from LoRA adaptation: 1.4034e+10\n",
      "Percentage of Total FLOPs Budget:   0.023085 %\n",
      "\n",
      "For a final evaluation\n",
      "Total FLOPs for evaluation: 1.8468e+14\n",
      "Total FLOPs from LoRA adaptation: 1.1227e+11\n",
      "Percentage of Total FLOPs Budget:   0.18468 %\n",
      "\n",
      "For a Total evaluation:\n",
      "3.878e+15\n",
      "Total FLOPs:\n",
      "4.820e+16\n",
      "Percentage of Budget used: 48.201%\n"
     ]
    }
   ],
   "source": [
    "# Training if no early stopping\n",
    "print('For all potential training steps (4000)')\n",
    "total_flops, _ = model_training_flops(no_tokens = 768, lora_ranks = 4, batch_size = 4, num_steps_training = 4000)\n",
    "# Evaluation\n",
    "print('')\n",
    "print('For a single mid training evaluation (repeated up to 160 times)')\n",
    "valuation_flops_inter, _ = model_evaluation_flops(no_tokens = 768, lora_ranks = 4 , batch_size = 25)\n",
    "print('')\n",
    "print('For a final evaluation')\n",
    "valuation_flops_final, _ = model_evaluation_flops(no_tokens = 768, lora_ranks = 4 , batch_size = 200)\n",
    "print('')\n",
    "print('For a Total evaluation:')\n",
    "print(f\"{(160 * valuation_flops_inter + valuation_flops_final):.3e}\")\n",
    "print(f\"Total FLOPs:\")\n",
    "print(f\"{(160 * valuation_flops_inter + valuation_flops_final + total_flops):.3e}\")\n",
    "print(f\"Percentage of Budget used: {((160 * valuation_flops_inter + valuation_flops_final + total_flops) / 1e17 * 100):.3f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "- Same calculations as the intial model\n",
    "\n",
    "### Cross Entropy Loss (0.2414%)\n",
    "### Autoregressive Generation - Very High not included"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (M2.5 Coursework)",
   "language": "python",
   "name": "m3cw5_py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
